# Audio Transcription & OCR System

Automated transcription using Faster-Whisper (GPU-accelerated) and OCR for PDFs with Norwegian and English support.

## ğŸš€ Quick Start

### 1. Install System Dependencies

```bash
# Update package list
sudo apt-get update

# Install required packages
sudo apt-get install -y \
    ffmpeg \
    tesseract-ocr \
    tesseract-ocr-eng \
    tesseract-ocr-nor \
    poppler-utils \
    python3-dev \
    python3-venv

# Or use the automated installer
chmod +x install.sh
./install.sh
```

### 2. Setup Project

```bash
# Make the script executable
chmod +x run_transcribe.sh

# Create input folder with nested structure
mkdir -p input/meeting1 input/meeting2

# Run the script (it will setup everything automatically)
./run_transcribe.sh
```

### 3. Configure Settings

Edit `config.yaml` to customize:
- Input/output folders (supports nested folders!)
- Whisper model size (large-v3 recommended for best quality)
- Audio enhancement settings
- Output formats (txt, srt, vtt, json)
- OCR settings
- Multi-pass transcription (for difficult audio)

### 4. Add Files and Run

```bash
# Add audio files and PDFs to nested folders
cp audio.mp3 input/meeting1/
cp document.pdf input/meeting1/
cp audio2.mp3 input/meeting2/

# Run transcription (handles all folders recursively!)
./run_transcribe.sh

# Or use multi-pass for difficult audio
./run_transcribe.sh config_multipass.yaml
```

---

## ğŸµ Audio Quality Improvement Guide

### Three-Stage Enhancement Pipeline

The script uses a powerful 3-stage enhancement pipeline:

1. **Stage 1: AI Denoising** (Demucs + DeepFilterNet)
   - Demucs separates vocals from background noise/music
   - DeepFilterNet applies NVIDIA-style noise suppression
   - Can use both sequentially for very noisy audio

2. **Stage 2: FFmpeg Filters** (Traditional DSP)
   - High-pass filter removes rumble
   - Adaptive noise reduction
   - Audio normalization

3. **Stage 3: Whisper VAD** (Voice Activity Detection)
   - Built into Whisper model
   - Removes silence automatically

### Configuration Examples

#### Maximum Quality (Very Noisy Audio)
```yaml
audio:
  ai_denoise:
    enabled: true
    method: "both"  # Try Demucs, then DeepFilterNet
  enhance: true
  enhancement_filters:
    noise_reduction: 0.15
```

#### Balanced (Most Use Cases)
```yaml
audio:
  ai_denoise:
    enabled: true
    method: "demucs"  # Vocal separation only
  enhance: true
```

#### Fast (Clean Audio)
```yaml
audio:
  ai_denoise:
    enabled: false
  enhance: true
  enhancement_filters:
    noise_reduction: 0.1
```

### AI Denoising Options

**Demucs** - Best for:
- Audio with background music
- Multiple speakers
- Environmental noise
- Isolating speech from other sounds

**DeepFilterNet** - Best for:
- Stationary noise (hum, hiss, fan noise)
- Wind noise
- Electronic interference
- Very aggressive noise suppression

**Both** - Best for:
- Very challenging audio
- Unknown noise types
- Maximum quality (slower processing)

### Whisper Model Selection for Quality

| Model | Speed | Accuracy | VRAM | Best For |
|-------|-------|----------|------|----------|
| tiny | Very Fast | Low | 1GB | Quick drafts |
| base | Fast | Medium | 1GB | Testing |
| small | Medium | Good | 2GB | Balanced |
| medium | Slow | Very Good | 5GB | High quality |
| **large-v3** | **Slowest** | **Best** | **10GB** | **Production** |

Your RTX 3080 (10GB VRAM) can handle **large-v3** perfectly!

### Optimal Settings for Norwegian Audio

```yaml
whisper:
  model_size: "large-v3"
  language: null  # Auto-detect (works great for NO/EN mix)
  beam_size: 5  # Increase to 8-10 for better accuracy (slower)
  vad_filter: true  # Removes silence/noise
  vad_parameters:
    threshold: 0.5  # Lower = more sensitive (0.3-0.7)
```

### Troubleshooting Poor Transcriptions

1. **Wrong Language Detected**: Set `language: "no"` explicitly
2. **Missing Words**: Increase `beam_size` to 8-10
3. **Too Much Noise**: Increase `noise_reduction` to 0.4-0.6
4. **Hallucinations**: Enable `vad_filter` and adjust threshold
5. **Low Volume**: Ensure `normalize: true` is enabled

---

## ğŸ“Š Output Formats

### Text with Timestamps + Language Tag (Default)
```
[Language: NO (confidence: 98.5%)]

[00:00:00] Velkommen til denne podcasten
[00:00:05] I dag skal vi diskutere kunstig intelligens
```

### Markdown Summary (Generated by Ollama)
```markdown
# Summary: podcast_episode

**Generated by:** llama3.2
**Language:** NO

This podcast episode discusses artificial intelligence and 
its applications in modern technology. The host covers 
three main topics: machine learning basics, neural networks, 
and practical AI implementations.

**Key Topics:**
- Introduction to AI concepts
- Neural network architectures  
- Real-world applications
- Future trends in AI

---
*Generated from transcription: podcast_episode.txt*
```

### SRT Subtitles
```
1
00:00:00,000 --> 00:00:05,000
Velkommen til denne podcasten

2
00:00:05,000 --> 00:00:10,000
I dag skal vi diskutere kunstig intelligens
```

### JSON (Full Metadata)
```json
{
  "language": "no",
  "language_probability": 0.98,
  "segments": [...],
  "full_text": "..."
}
```

---

## ğŸ¤– Ollama Integration

### Available Models

The script will list available models on your system. Popular options:

- **llama3.2** - Fast, good quality (recommended)
- **llama3.1** - Larger, better for complex summaries
- **mistral** - Excellent for multilingual (NO/EN)
- **gemma2** - Good balance of speed/quality
- **qwen2.5** - Strong multilingual support

### Install Models

```bash
# Check what's installed
ollama list

# Pull a model
ollama pull llama3.2
ollama pull mistral

# Test a model
ollama run llama3.2 "Test in Norwegian"
```

### Summary Styles

Configure in `config.yaml`:

```yaml
ollama:
  summary:
    style: "detailed"  # Options: concise, detailed, bullet_points
    max_length: 500    # Words
    extract_topics: true
    language: null     # null = same as audio, or "en", "no"
```

### GPU Usage Note

Since you have 1 GPU:
- Whisper transcription uses GPU
- When transcription finishes, Ollama can use GPU for summary
- With `async_processing: true`, summarization happens after transcription
- This prevents GPU memory conflicts

If Ollama is running during transcription, it will use some VRAM but shouldn't cause issues with large-v3 on 10GB RTX 3080.

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ config.yaml              # Standard configuration
â”œâ”€â”€ config_multipass.yaml    # Multi-pass for difficult audio
â”œâ”€â”€ transcribe.py            # Main Python script
â”œâ”€â”€ run_transcribe.sh        # Launcher script
â”œâ”€â”€ install.sh               # Automated installer
â”œâ”€â”€ check_system.sh          # System checker
â”œâ”€â”€ select_ollama_model.sh   # Model switcher
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ input/                   # Nested folders supported!
â”‚   â”œâ”€â”€ meeting1/
â”‚   â”‚   â”œâ”€â”€ audio1.mp3
â”‚   â”‚   â””â”€â”€ document.pdf
â”‚   â”œâ”€â”€ meeting2/
â”‚   â”‚   â”œâ”€â”€ audio2.wav
â”‚   â”‚   â””â”€â”€ notes.pdf
â”‚   â””â”€â”€ kvitteringer/
â”‚       â”œâ”€â”€ doc1.pdf
â”‚       â””â”€â”€ doc2.pdf
â””â”€â”€ output/                  # Optional: preserves structure
    â”œâ”€â”€ meeting1/
    â”‚   â”œâ”€â”€ audio1.txt       # Transcription
    â”‚   â”œâ”€â”€ audio1.md        # Summary
    â”‚   â””â”€â”€ document.txt     # OCR
    â””â”€â”€ meeting2/
        â”œâ”€â”€ audio2.txt
        â”œâ”€â”€ audio2.md
        â””â”€â”€ notes.txt
```

---

## ğŸ”„ Multi-Pass Transcription (NEW!)

For difficult/noisy audio, transcribe multiple times with different strategies and use AI to merge the best results.

### Quick Start
```bash
./run_transcribe.sh config_multipass.yaml
```

### What It Does
1. Transcribes audio 4 times:
   - No AI denoising (baseline)
   - Demucs (vocal separation)
   - DeepFilterNet (noise suppression)  
   - Both (maximum processing)
2. Ollama AI compares all versions at each timestamp
3. Picks the most accurate text
4. Creates merged "best of all" transcription

### Output
```
meeting1/
â”œâ”€â”€ opptak.txt              # â­ LLM-merged best version
â”œâ”€â”€ opptak_none.txt         # Individual versions for comparison
â”œâ”€â”€ opptak_demucs.txt
â”œâ”€â”€ opptak_deepfilternet.txt
â”œâ”€â”€ opptak_both.txt
â””â”€â”€ opptak.md               # Summary
```

**Perfect for "fire and forget" overnight processing of 20+ meetings!**

See `BATCH_PROCESSING.md` for complete guide.

---

## âš™ï¸ Configuration Examples

### Maximum Quality (Slow, Best Results)
```yaml
whisper:
  model_size: "large-v3"
  beam_size: 10

audio:
  ai_denoise:
    enabled: true
    method: "both"
  enhance: true

ollama:
  enabled: true
  model: "llama3.1"  # Larger model
  summary:
    style: "detailed"
    max_length: 800
```

### Fast Processing (Good Quality)
```yaml
whisper:
  model_size: "medium"
  beam_size: 5

audio:
  ai_denoise:
    enabled: true
    method: "demucs"
  enhance: true

ollama:
  enabled: true
  model: "llama3.2"
  summary:
    style: "concise"
    max_length: 300
```

### Norwegian-Only, Maximum Quality
```yaml
whisper:
  language: "no"
  model_size: "large-v3"
  beam_size: 8

ocr:
  language: "nor"

ollama:
  summary:
    language: "no"
    style: "detailed"
```

### Dual Output (English Summary of Norwegian Audio)
```yaml
whisper:
  language: null  # Auto-detect

ollama:
  summary:
    language: "en"  # Summarize in English
    style: "detailed"
```

---

## ğŸ”§ Advanced Usage

### Process Specific Config
```bash
./run_transcribe.sh custom_config.yaml
```

### Monitor GPU Usage
```bash
watch -n 1 nvidia-smi
```

### Process Single File (Python)
```python
from transcribe import TranscriptionProcessor

processor = TranscriptionProcessor()
results = processor.transcribe_audio(Path("audio.mp3"))
processor.save_transcription(results, Path("output.txt"))
```

---

## ğŸ“ Tips

- **RTX 3080 Performance**: Can process ~20-30x realtime with large-v3
- **Batch Processing**: Handles nested folders automatically - perfect for "fire and forget"
- **Multi-Pass**: For difficult audio, try 4 strategies and merge best results with AI
- **Resume Support**: Set `skip_existing: true` to resume interrupted jobs
- **Audio Formats**: Supports MP3, WAV, M4A, FLAC, OGG, OPUS, AAC, MP4, WEBM
- **Long Files**: No length limit - Whisper handles hours of audio
- **Folder Structure**: Automatically preserved in output (20 meetings â†’ 20 folders with results)
- **Progress Monitoring**: Use `./monitor_progress.sh input` to track batch jobs
- **Overnight Processing**: 20 hours of audio â†’ ~2 hours (standard) or ~8 hours (multi-pass)

---

## ğŸ¯ Real-World Example

```bash
# Your 20 meeting folders
input/
â”œâ”€â”€ meeting1/opptak.m4a, refferat.pdf
â”œâ”€â”€ meeting2/opptak.m4a, refferat.pdf
â”œâ”€â”€ meeting3/opptak.m4a, refferat.pdf
... (20 total)

# Run multi-pass before bed
./run_transcribe.sh config_multipass.yaml &

# Monitor in another terminal
./monitor_progress.sh input

# Wake up to:
input/
â”œâ”€â”€ meeting1/
â”‚   â”œâ”€â”€ opptak.m4a
â”‚   â”œâ”€â”€ opptak.txt (best transcription)
â”‚   â”œâ”€â”€ opptak_none.txt (comparison)
â”‚   â”œâ”€â”€ opptak_demucs.txt
â”‚   â”œâ”€â”€ opptak_deepfilternet.txt
â”‚   â”œâ”€â”€ opptak_both.txt
â”‚   â”œâ”€â”€ opptak.md (AI summary)
â”‚   â”œâ”€â”€ refferat.pdf
â”‚   â””â”€â”€ refferat.txt (OCR)
... (all 20 meetings done!)
```

**Perfect for batch processing overnight!** ğŸŒ™â†’â˜•

---

## ğŸ› Common Issues

### "CUDA out of memory"
- Reduce model size to "medium" or "small"
- Close other GPU applications

### "Tesseract not found"
```bash
sudo apt-get install tesseract-ocr
```

### "No such file: config.yaml"
- Ensure config.yaml is in the same directory as the script

### Poor Norwegian Recognition
- Ensure `tesseract-ocr-nor` is installed for OCR
- For Whisper, language auto-detection works well

---

## ğŸ“š Documentation

- Faster-Whisper: https://github.com/SYSTRAN/faster-whisper
- Whisper Models: https://github.com/openai/whisper
- FFmpeg Filters: https://ffmpeg.org/ffmpeg-filters.html
- Tesseract OCR: https://github.com/tesseract-ocr/tesseract

---

**Made with â¤ï¸ for accurate Norwegian & English transcription**